import pandas as pd
import numpy as np
from sklearn.model_selection import KFold, cross_val_score, RandomizedSearchCV, train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder
from sklearn.preprocessing import OneHotEncoder
from sklearn.ensemble import RandomForestRegressor
from sklearn.impute import KNNImputer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from imblearn.over_sampling import SMOTE
import holidays
from datetime import datetime
import logging
import warnings
from typing import Dict, List, Tuple, Union
warnings.filterwarnings('ignore')

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class EnhancedAccidentPredictorV3:
    """
    Enhanced accident prediction model with advanced preprocessing, feature engineering,
    and model tuning capabilities.
    """
    
    def __init__(self):
        """Initialize the enhanced accident prediction model with advanced components"""
        self._initialize_feature_groups()
        self._initialize_preprocessors()  # Ensure this method is defined
        self._initialize_model()
        
        # Initialize US holidays
        self.us_holidays = holidays.US()
        
        # Track important features
        self.important_features = None
        self.feature_importance_threshold = 0.01
        
        # Store preprocessing states
        self.fitted_preprocessors = False
        self.fitted_model = False
        
        # Store column names for verification
        self.expected_columns = None
    
    def _initialize_feature_groups(self):
        """Initialize feature group definitions for preprocessing"""
        # Numeric features that need scaling
        self.numeric_features = [
            'Temperature(F)', 'Humidity(%)', 'Pressure(in)', 
            'Visibility(mi)', 'Wind_Speed(mph)', 'Distance(mi)'
        ]
        
        # Categorical features with high cardinality (use target encoding)
        self.high_cardinality_features = ['City', 'County']
        
        # Categorical features with low cardinality (use one-hot encoding)
        self.low_cardinality_features = ['State', 'Weather_Condition']
        
        # Binary features (convert to int)
        self.binary_features = [
            'Crossing', 'Junction', 'Traffic_Signal'
        ]
        
        # Generated time features
        self.time_features = [
            'hour', 'day_of_week', 'month',
            'is_night', 'is_weekend', 'is_rush_hour', 
            'is_holiday'
        ]
        
        # Columns to drop
        self.columns_to_drop = [
            'End_Lat', 'End_Lng', 'Wind_Chill(F)',
            'Precipitation(in)', 'Wind_Direction'
        ]
    
    def _initialize_preprocessors(self):
        """Initialize all preprocessing components"""
        # Imputers
        self.numeric_imputer = KNNImputer(
            n_neighbors=5,
            weights='distance'
        )
        
        # Encoders
        self.high_cardinality_encoder = {
            feature: LabelEncoder() for feature in self.high_cardinality_features
        }
        
        self.low_cardinality_encoder = OneHotEncoder(
            sparse=False,
            handle_unknown='ignore'
        )
        
        # Scaler for numeric features
        self.scaler = StandardScaler()
    
    def _initialize_model(self):
        """Initialize the Random Forest model with default parameters"""
        self.model = RandomForestRegressor(
            n_estimators=200,
            random_state=42,
            n_jobs=-1
        )
    
    # Add the rest of your methods here (e.g., preprocess_data, train, etc.)

    def preprocess_data(self, df: pd.DataFrame, train_mode: bool = True) -> pd.DataFrame:
        """
        Preprocess all features for model training or prediction.
        """
        print("Starting preprocessing...")
        
        try:
            # Create a deep copy to avoid modifying original data
            df_processed = df.copy()
            df_processed = df_processed.reset_index(drop=True)
            
            # Columns to remove
            columns_to_remove = [
                'ID', 'Source', 'End_Time', 'End_Lat', 'End_Lng', 
                'Description', 'Country', 'Timezone', 'Weather_Timestamp'
            ]
            
            # Drop specified columns
            df_processed = df_processed.drop(columns=columns_to_remove, errors='ignore')
            
            # Create time features first
            df_processed = self._create_time_features(df_processed)
            
            # Drop the original Start_Time column
            df_processed = df_processed.drop(columns=['Start_Time'], errors='ignore')
            
            # Separate numeric and categorical columns
            numeric_features = [
                'Temperature(F)', 'Humidity(%)', 'Pressure(in)', 
                'Visibility(mi)', 'Wind_Speed(mph)', 'Distance(mi)',
                'hour', 'day_of_week', 'month', 'is_weekend',
                'is_rush_hour', 'is_holiday', 'is_night'
            ]
            
            categorical_features = [
                'City', 'County', 'State', 'Weather_Condition',
                'Crossing', 'Junction', 'Traffic_Signal'
            ]
            
            # Keep only the columns we need
            needed_columns = numeric_features + categorical_features
            df_processed = df_processed[needed_columns].copy()
            
            # Convert boolean columns to int
            bool_columns = ['Crossing', 'Junction', 'Traffic_Signal']
            for col in bool_columns:
                if col in df_processed.columns:
                    df_processed[col] = df_processed[col].astype(int)
            
            # Handle numeric features
            numeric_data = df_processed[numeric_features].copy()
            
            # Apply KNN Imputer to numeric columns
            if train_mode:
                imputed_numeric_data = self.numeric_imputer.fit_transform(numeric_data)
            else:
                imputed_numeric_data = self.numeric_imputer.transform(numeric_data)
            
            # Convert imputed data back to DataFrame
            numeric_df = pd.DataFrame(
                imputed_numeric_data,
                columns=numeric_features,
                index=df_processed.index
            )
            
            # Handle categorical features
            categorical_data = df_processed[categorical_features].copy()
            
            # Fill missing values in categorical columns
            for col in categorical_features:
                categorical_data[col] = categorical_data[col].fillna('Unknown')
            
            # Encode categorical columns
            if train_mode:
                self.one_hot_encoder = OneHotEncoder(
                    handle_unknown='ignore', 
                    sparse=False
                )
                encoded_data = self.one_hot_encoder.fit_transform(categorical_data)
            else:
                encoded_data = self.one_hot_encoder.transform(categorical_data)
            
            # Convert encoded data to DataFrame
            encoded_df = pd.DataFrame(
                encoded_data,
                columns=self.one_hot_encoder.get_feature_names_out(categorical_features),
                index=df_processed.index
            )
            
            # Combine numeric and encoded categorical data
            final_df = pd.concat([numeric_df, encoded_df], axis=1)
            
            print("Preprocessing completed successfully")
            return final_df
            
        except Exception as e:
            print(f"Error in preprocess_data: {str(e)}")
            print(f"Available columns: {df_processed.columns.tolist()}")
            raise
    
    def _validate_and_clean_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Validate input data and handle missing values consistently.
        
        This method serves as a pre-preprocessing step to ensure data quality
        and consistency before the main preprocessing pipeline.
        """
        print("Starting data validation and cleaning")
        df_cleaned = df.copy()
        
        # Check for required columns
        required_columns = (
            self.numeric_features +
            self.high_cardinality_features +
            self.low_cardinality_features +
            self.binary_features
        )
        
        missing_columns = [col for col in required_columns if col not in df_cleaned.columns]
        if missing_columns:
            raise ValueError(f"Missing required columns: {missing_columns}")
        
        # Handle missing values for each feature type
        for feature in self.numeric_features:
            if df_cleaned[feature].isna().any():
                print(f"Imputing missing values for {feature}")
                # Use median for initial filling to enable model training
                df_cleaned[feature] = df_cleaned[feature].fillna(
                    df_cleaned[feature].median()
                )
        
        for feature in self.high_cardinality_features:
            df_cleaned[feature] = df_cleaned[feature].fillna('MISSING')
            
        for feature in self.low_cardinality_features:
            df_cleaned[feature] = df_cleaned[feature].fillna('MISSING')
            
        for feature in self.binary_features:
            df_cleaned[feature] = df_cleaned[feature].fillna(False)
        
        return df_cleaned  
 
    def _preprocess_numeric_features(self, df: pd.DataFrame, train_mode: bool = True) -> np.ndarray:
        """
        Preprocess numeric features with imputation and scaling
        
        Args:
            df: Input dataframe
            train_mode: Whether in training or prediction mode
            
        Returns:
            Processed numeric features as numpy array
        """
        numeric_data = df[self.numeric_features].copy()
        
        if train_mode:
            numeric_data = self.numeric_imputer.fit_transform(numeric_data)
            numeric_data = self.scaler.fit_transform(numeric_data)
        else:
            numeric_data = self.numeric_imputer.transform(numeric_data)
            numeric_data = self.scaler.transform(numeric_data)
        
        return numeric_data
    
    def _preprocess_categorical_features(
        self, 
        df: pd.DataFrame, 
        train_mode: bool = True
    ) -> Tuple[np.ndarray, np.ndarray]:
        """
        Preprocess categorical features with robust handling of unknown categories
        
        Args:
            df: Input dataframe
            train_mode: Whether in training or prediction mode
            
        Returns:
            Tuple of processed high and low cardinality features
        """
        # Handle high cardinality features with label encoding and unknown value handling
        high_card_encoded = np.zeros((len(df), len(self.high_cardinality_features)))
        
        for idx, feature in enumerate(self.high_cardinality_features):
            feature_values = df[feature].fillna('MISSING')
            
            if train_mode:
                # During training, fit the encoder with an additional 'UNKNOWN' category
                unique_values = np.append(feature_values.unique(), 'UNKNOWN')
                self.high_cardinality_encoder[feature].fit(unique_values)
                high_card_encoded[:, idx] = self.high_cardinality_encoder[feature].transform(feature_values)
            else:
                # During prediction, map unseen categories to 'UNKNOWN'
                known_categories = set(self.high_cardinality_encoder[feature].classes_)
                feature_values = feature_values.map(
                    lambda x: x if x in known_categories else 'UNKNOWN'
                )
                try:
                    high_card_encoded[:, idx] = self.high_cardinality_encoder[feature].transform(feature_values)
                except Exception as e:
                    print(f"Error encoding feature {feature}: {str(e)}")
                    # Use the encoding for 'UNKNOWN' as fallback
                    high_card_encoded[:, idx] = self.high_cardinality_encoder[feature].transform(['UNKNOWN'] * len(df))
        
        # Handle low cardinality features with one-hot encoding
        if train_mode:
            low_card_encoded = self.low_cardinality_encoder.fit_transform(
                df[self.low_cardinality_features].fillna('MISSING')
            )
        else:
            # OneHotEncoder already handles unknown categories when handle_unknown='ignore'
            low_card_encoded = self.low_cardinality_encoder.transform(
                df[self.low_cardinality_features].fillna('MISSING')
            )
        
        return high_card_encoded, low_card_encoded    
    def _create_time_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Extract and encode enhanced time-based features
        """
        df_copy = df.copy()
        
        try:
            # Convert Start_Time to datetime if needed
            if 'Start_Time' not in df_copy.columns:
                print("Warning: Start_Time column not found")
                # Create default time features
                df_copy['hour'] = 0
                df_copy['day_of_week'] = 0
                df_copy['month'] = 1
                df_copy['is_weekend'] = 0
                df_copy['is_rush_hour'] = 0
                df_copy['is_holiday'] = 0
                df_copy['is_night'] = 0
                return df_copy
                
            df_copy['Start_Time'] = pd.to_datetime(df_copy['Start_Time'], errors='coerce')
            
            # Fill missing timestamps with a default value
            df_copy['Start_Time'] = df_copy['Start_Time'].fillna(pd.Timestamp('2023-01-01 00:00:00'))
            
            # Extract time components
            df_copy['hour'] = df_copy['Start_Time'].dt.hour
            df_copy['day_of_week'] = df_copy['Start_Time'].dt.dayofweek
            df_copy['month'] = df_copy['Start_Time'].dt.month
            
            # Create binary indicators
            df_copy['is_weekend'] = df_copy['day_of_week'].isin([5, 6]).astype(int)
            df_copy['is_rush_hour'] = (
                ((df_copy['hour'] >= 7) & (df_copy['hour'] <= 9)) |
                ((df_copy['hour'] >= 16) & (df_copy['hour'] <= 18))
            ).astype(int)
            
            df_copy['is_holiday'] = df_copy['Start_Time'].dt.date.map(
                lambda x: x in self.us_holidays
            ).astype(int)
            
            df_copy['is_night'] = (
                (df_copy['hour'] >= 20) | (df_copy['hour'] <= 5)
            ).astype(int)
            
            return df_copy
            
        except Exception as e:
            print(f"Error in _create_time_features: {str(e)}")
            print(f"DataFrame columns: {df_copy.columns.tolist()}")
            raise   
    def create_target_variable(self, df: pd.DataFrame) -> np.ndarray:
        """
        Create target variable for training
        """
        # Create a deep copy to avoid modifying the original dataframe
        df_copy = df.copy()
        
        # Ensure time features exist
        if 'is_rush_hour' not in df_copy.columns:
            df_copy = self._create_time_features(df_copy)
            
        # Create severity score with proper index alignment
        severity_score = pd.Series(
            data=(
                df_copy['Distance(mi)'].fillna(0) * 0.3 +
                (df_copy['Visibility(mi)'].fillna(10).max() - df_copy['Visibility(mi)'].fillna(10)) * 0.2 +
                (df_copy['Wind_Speed(mph)'].fillna(0) > 20).astype(int) * 0.2 +
                df_copy['is_rush_hour'].astype(int) * 0.15 +
                df_copy['is_night'].astype(int) * 0.15
            ),
            index=df_copy.index
        )
        
        return severity_score.values
    def _tune_hyperparameters(self, X: np.ndarray, y: np.ndarray):
        """
        Tune model hyperparameters using RandomizedSearchCV
        
        Args:
            X: Feature matrix
            y: Target variable
        """
        print("Starting hyperparameter tuning")
        
        try:
            # Define expanded parameter space
            param_dist = {
                'n_estimators': [100, 200, 300, 400, 500],
                'max_depth': [10, 20, 30, 40, 50, None],
                'min_samples_split': [2, 5, 10, 15, 20],
                'min_samples_leaf': [1, 2, 4, 6, 8],
                'max_features': ['auto', 'sqrt', 'log2'],
                'bootstrap': [True, False],
                'max_samples': [0.7, 0.8, 0.9, None]
            }
            
            # Initialize RandomizedSearchCV with cross-validation
            random_search = RandomizedSearchCV(
                self.model,
                param_distributions=param_dist,
                n_iter=50,  # Increased number of iterations
                cv=5,
                scoring='neg_mean_squared_error',
                random_state=42,
                n_jobs=-1
            )
            
            # Fit RandomizedSearchCV
            random_search.fit(X, y)
            
            # Update model with best parameters
            self.model = random_search.best_estimator_
            print(f"Best parameters found: {random_search.best_params_}")
            
        except Exception as e:
            print(f"Error during hyperparameter tuning: {str(e)}")
            print("Using default hyperparameters")
    
    def train(self, df: pd.DataFrame) -> Dict[str, float]:
        """
        Train the enhanced accident prediction model
        """
        print("Starting model training process")
        
        try:
            # Initial data validation and cleaning
            df_cleaned = self._validate_and_clean_data(df)
            
            # Create target variable
            y = self.create_target_variable(df_cleaned)
            
            # Preprocess features
            X = self.preprocess_data(df_cleaned, train_mode=True)
            
            # Verify sample size consistency
            if len(X) != len(y):
                raise ValueError(
                    f"Inconsistent sample sizes: X has {len(X)} samples, "
                    f"y has {len(y)} samples"
                )
            
            # Remove any remaining rows with NaN values
            mask = ~(X.isna().any(axis=1) | pd.isna(y))
            X = X[mask]
            y = y[mask]
            
            # Convert to numpy arrays for sklearn
            X = X.values
            y = np.array(y)
            
            print(f"Final training set size: {len(X)} samples")
            
            # Train the model
            self.model.fit(X, y)
            self.fitted_model = True
            
            # Calculate and return metrics
            predictions = self.model.predict(X)
            metrics = {
                'r2_score': r2_score(y, predictions),
                'mae': mean_absolute_error(y, predictions),
                'rmse': np.sqrt(mean_squared_error(y, predictions)),
                'explained_variance': np.var(predictions) / np.var(y),
                'training_samples': len(X)
            }
            
            print("Model training completed successfully")
            return metrics
            
        except Exception as e:
            print(f"Error during model training: {str(e)}")
            raise    
    def predict_risk_score(self, new_data: pd.DataFrame) -> np.ndarray:
        """
        Predict accident risk scores
        
        Args:
            new_data: New data for prediction
            
        Returns:
            Predicted risk scores (0-1 scale)
        """
        if not self.fitted_model:
            raise ValueError("Model must be trained before making predictions")
        
        try:
            print("Starting prediction process")
            
            # Preprocess the new data
            X_new = self.preprocess_data(new_data, train_mode=False)
            
            # Use only important features if available
            if self.important_features:
                X_new = X_new[self.important_features]
            
            # Make predictions
            predictions = self.model.predict(X_new)
            
            # Convert to risk scores (0-1 scale)
            risk_scores = (predictions - predictions.min()) / (
                predictions.max() - predictions.min()
            )
            
            print("Predictions generated successfully")
            return risk_scores
            
        except Exception as e:
            print(f"Error during prediction: {str(e)}")
            raise
    
    def _select_important_features(self, X: pd.DataFrame) -> List[str]:
        """
        Select important features based on feature importance scores with dynamic thresholding
        
        Args:
            X: Feature matrix with column names
            
        Returns:
            List of important feature names
        """
        try:
            # Get feature importances
            importances = self.model.feature_importances_
            
            # Create dictionary of feature importances
            feature_importance = dict(zip(X.columns, importances))
            
            # Sort features by importance
            sorted_features = sorted(
                feature_importance.items(),
                key=lambda x: x[1],
                reverse=True
            )
            
            # Dynamic thresholding: Keep features that contribute to 95% of cumulative importance
            cumulative_importance = 0
            selected_features = []
            
            for feature, importance in sorted_features:
                cumulative_importance += importance
                selected_features.append(feature)
                
                if cumulative_importance >= 0.95:
                    break
            
            # Additional filter: Ensure minimum importance threshold
            selected_features = [
                feature for feature in selected_features
                if feature_importance[feature] > self.feature_importance_threshold
            ]
            
            print(f"Selected {len(selected_features)} important features")
            
            # Log top 10 most important features
            print("Top 10 most important features:")
            for feature, importance in sorted_features[:10]:
                print(f"{feature}: {importance:.4f}")
            
            return selected_features
            
        except Exception as e:
            print(f"Error selecting important features: {str(e)}")
            return list(X.columns)
    
    def evaluate_model(self, test_data: pd.DataFrame) -> Dict[str, float]:
        """
        Evaluate model performance on test data with comprehensive metrics
        
        Args:
            test_data: Test dataset
            
        Returns:
            Dictionary of evaluation metrics
        """
        if not self.fitted_model:
            raise ValueError("Model must be trained before evaluation")
        
        try:
            print("Starting model evaluation")
            
            # Preprocess test data
            X_test = self.preprocess_data(test_data, train_mode=False)
            y_test = self.create_target_variable(test_data)
            
            # Use important features if available
            if self.important_features:
                X_test = X_test[self.important_features]
            
            # Generate predictions
            predictions = self.model.predict(X_test)
            
            # Calculate comprehensive metrics
            metrics = {
                'r2_score': r2_score(y_test, predictions),
                'mae': mean_absolute_error(y_test, predictions),
                'rmse': np.sqrt(mean_squared_error(y_test, predictions)),
                'explained_variance': np.var(predictions) / np.var(y_test),
                'mean_prediction': np.mean(predictions),
                'std_prediction': np.std(predictions)
            }
            
            # Calculate prediction intervals using bootstrapping
            prediction_intervals = self._calculate_prediction_intervals(X_test)
            
            metrics.update({
                'mean_interval_width': np.mean(
                    prediction_intervals[:, 1] - prediction_intervals[:, 0]
                ),
                'coverage_probability': np.mean(
                    (y_test >= prediction_intervals[:, 0]) &
                    (y_test <= prediction_intervals[:, 1])
                )
            })
            
            print("Model evaluation completed")
            print("\nEvaluation Metrics:")
            for metric, value in metrics.items():
                print(f"{metric}: {value:.4f}")
            
            return metrics
            
        except Exception as e:
            print(f"Error during model evaluation: {str(e)}")
            raise
    
    def _calculate_prediction_intervals(
        self,
        X: pd.DataFrame,
        n_bootstraps: int = 100,
        confidence_level: float = 0.95
    ) -> np.ndarray:
        """
        Calculate prediction intervals using bootstrap sampling
        
        Args:
            X: Feature matrix
            n_bootstraps: Number of bootstrap samples
            confidence_level: Confidence level for intervals
            
        Returns:
            Array of lower and upper bounds for each prediction
        """
        predictions = []
        n_samples = len(X)
        
        # Generate bootstrap predictions
        for _ in range(n_bootstraps):
            # Sample with replacement
            indices = np.random.randint(0, n_samples, n_samples)
            sample_weight = np.bincount(indices, minlength=n_samples)
            
            # Train a new model on the bootstrap sample
            bootstrap_model = RandomForestRegressor(**self.model.get_params())
            bootstrap_model.fit(X, np.zeros(len(X)), sample_weight=sample_weight)
            
            # Make predictions
            predictions.append(bootstrap_model.predict(X))
        
        # Calculate intervals
        predictions = np.array(predictions)
        lower_percentile = (1 - confidence_level) / 2
        upper_percentile = 1 - lower_percentile
        
        intervals = np.percentile(
            predictions,
            [lower_percentile * 100, upper_percentile * 100],
            axis=0
        ).T
        
        return intervals
    
    def save_model(self, filepath: str):
        """
        Save the trained model and preprocessors to disk
        
        Args:
            filepath: Path to save the model
        """
        try:
            import joblib
            
            model_data = {
                'model': self.model,
                'numeric_imputer': self.numeric_imputer,
                'high_cardinality_encoder': self.high_cardinality_encoder,
                'low_cardinality_encoder': self.low_cardinality_encoder,
                'scaler': self.scaler,
                'important_features': self.important_features,
                'feature_groups': {
                    'numeric_features': self.numeric_features,
                    'high_cardinality_features': self.high_cardinality_features,
                    'low_cardinality_features': self.low_cardinality_features,
                    'binary_features': self.binary_features,
                    'time_features': self.time_features
                }
            }
            
            joblib.dump(model_data, filepath)
            print(f"Model saved successfully to {filepath}")
            
        except Exception as e:
            print(f"Error saving model: {str(e)}")
            raise
    
    @classmethod
    def load_model(cls, filepath: str) -> 'EnhancedAccidentPredictorV3':
        """
        Load a trained model from disk
        
        Args:
            filepath: Path to the saved model
            
        Returns:
            Loaded model instance
        """
        try:
            import joblib
            
            # Create a new instance
            instance = cls()
            
            # Load the saved data
            model_data = joblib.load(filepath)
            
            # Restore model and preprocessors
            instance.model = model_data['model']
            instance.numeric_imputer = model_data['numeric_imputer']
            instance.high_cardinality_encoder = model_data['high_cardinality_encoder']
            instance.low_cardinality_encoder = model_data['low_cardinality_encoder']
            instance.scaler = model_data['scaler']
            instance.important_features = model_data['important_features']
            
            # Restore feature groups
            feature_groups = model_data['feature_groups']
            instance.numeric_features = feature_groups['numeric_features']
            instance.high_cardinality_features = feature_groups['high_cardinality_features']
            instance.low_cardinality_features = feature_groups['low_cardinality_features']
            instance.binary_features = feature_groups['binary_features']
            instance.time_features = feature_groups['time_features']
            
            # Set fitted flags
            instance.fitted_model = True
            instance.fitted_preprocessors = True
            
            print(f"Model loaded successfully from {filepath}")
            return instance
            
        except Exception as e:
            print(f"Error loading model: {str(e)}")
            raise

# Example usage
def main():
    try:
        # Load data
        df = pd.read_csv('/kaggle/input/mini-us-accident/downsized.csv')
        print("Data loaded successfully")
        
        # Reset index to ensure consistency
        df = df.reset_index(drop=True)
        
        # First create time features
        predictor = EnhancedAccidentPredictorV3()
        df = predictor._create_time_features(df)
        
        # Create train-test split
        train_indices, test_indices = train_test_split(
            range(len(df)), 
            test_size=0.2, 
            random_state=42
        )
        
        train_data = df.iloc[train_indices]
        test_data = df.iloc[test_indices]
        
        # Train the model and get training metrics
        print("\nTraining model...")
        training_metrics = predictor.train(train_data)
        
        print("\nTraining Metrics:")
        for metric, value in training_metrics.items():
            print(f"{metric}: {value:.4f}")
            
        # Evaluate on test set
        print("\nEvaluating model on test set...")
        test_metrics = predictor.evaluate_model(test_data)
        
        print("\nTest Metrics:")
        for metric, value in test_metrics.items():
            print(f"{metric}: {value:.4f}")
        
        # Example prediction
        print("\nMaking example prediction...")
        new_data = pd.DataFrame({
            'Start_Time': ['2023-01-01 08:00:00'],
            'Temperature(F)': [70],
            'Humidity(%)': [80],
            'Pressure(in)': [29.92],
            'Visibility(mi)': [10],
            'Wind_Speed(mph)': [5],
            'City': ['Dayton'],
            'County': ['Montgomery'],
            'State': ['OH'],
            'Weather_Condition': ['Clear'],
            'Crossing': [False],
            'Junction': [True],
            'Traffic_Signal': [True],
            'Distance(mi)': [0.5]
        })
        
        risk_scores = predictor.predict_risk_score(new_data)
        print(f"Predicted risk score for example data: {risk_scores[0]:.4f}")
        
        # Save model
        print("\nSaving model...")
        predictor.save_model('accident_predictor_model.joblib')
        print("Model saved successfully!")
        
        return predictor, training_metrics, test_metrics
        
    except Exception as e:
        print(f"Error in main execution: {str(e)}")
        import traceback
        print(traceback.format_exc())
        raise

if __name__ == "__main__":
    predictor, training_metrics, test_metrics = main()




    Data loaded successfully

Training model...
Starting model training process
Starting data validation and cleaning
Imputing missing values for Temperature(F)
Imputing missing values for Humidity(%)
Imputing missing values for Pressure(in)
Imputing missing values for Visibility(mi)
Imputing missing values for Wind_Speed(mph)
Starting preprocessing...
Preprocessing completed successfully
Final training set size: 8000 samples
Model training completed successfully

Training Metrics:
r2_score: 0.9905
mae: 0.0021
rmse: 0.0531
explained_variance: 0.9391
training_samples: 8000.0000

Evaluating model on test set...
Starting model evaluation
Starting preprocessing...
Preprocessing completed successfully
Model evaluation completed

Evaluation Metrics:
r2_score: -390.8351
mae: 10.0008
rmse: 10.0009
explained_variance: 0.9808
mean_prediction: 14.2955
std_prediction: 0.5004
mean_interval_width: 0.0000
coverage_probability: 0.0005

Test Metrics:
r2_score: -390.8351
mae: 10.0008
rmse: 10.0009
explained_variance: 0.9808
mean_prediction: 14.2955
std_prediction: 0.5004
mean_interval_width: 0.0000
coverage_probability: 0.0005

Making example prediction...
Starting prediction process
Starting preprocessing...
Preprocessing completed successfully
Predictions generated successfully
Predicted risk score for example data: nan